{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sagemaker demo\n",
    "\n",
    "Sagemaker has 3 options for training and deploying models:\n",
    "1. A standard Sagemaker model, like xgboost.\n",
    "2. A custom `sklearn` model using a pre-built sagemaker scikit-learn docker image.\n",
    "3. Any other custom build model using an own custom image\n",
    "\n",
    "In this notebook we demonstrate how to do option 1 and 2, which can be done without using a `Dockerfile`. Option 3 involves a bit more engineering.\n",
    "\n",
    "For part one we start by following this [step-by-step guide](https://aws.amazon.com/getting-started/tutorials/build-train-deploy-machine-learning-model-sagemaker/) on how to model with sagemaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "bucket_name = 'playbucket-steven'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example data\n",
    "\n",
    "Here we are going to predict Black Friday sales based on this [kaggle data set](https://www.kaggle.com/mehdidag/black-friday/version/1).\n",
    "- Its around 5 MB\n",
    "- Its dimensions are 538k x 12\n",
    "\n",
    "### Getting data from S3\n",
    "\n",
    "Boto is the AWS SDK for Python. Access S3 using boto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create s3 resource\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Print out bucket names\n",
    "for bucket in s3.buckets.all():\n",
    "    print(bucket.name)\n",
    "    # print out file names in bucket, called 'keys'\n",
    "    for objects in bucket.objects.all():\n",
    "        print('- ', objects.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon S3 does not provide compute, such as zip compression/decompression. You would need to write a program that:\n",
    "- Downloads the zip file\n",
    "- Extracts the files\n",
    "- Does actions on the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "# Download if not there yet\n",
    "if not os.path.isfile('../data/black-friday.zip'):\n",
    "    s3.Bucket(bucket_name).download_file('black-friday.zip', '../data/black-friday.zip')\n",
    "\n",
    "# Extract and read with pandas\n",
    "zf = zipfile.ZipFile('../data/black-friday.zip') \n",
    "df = pd.read_csv(zf.open('BlackFriday.csv'))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or just use the super nice pandas API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('s3://' + bucket_name + '/black-friday.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine and preprocess dataset\n",
    "\n",
    "Target column to predict: \t`Purchase` = Purchase amount in dollars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create X and y data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 7:] # keep only integer columns to avoid one hot encoding\n",
    "y = X.pop('Purchase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split in train test and create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Using a standard `Sagemaker` model\n",
    "\n",
    "To prepare the data, train the machine learning model, and deploy it, you will need to import some libraries and define a few environment variables in your Jupyter notebook environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import boto3, re, sys, math, json, os, sagemaker, urllib.request\n",
    "from sagemaker import get_execution_role\n",
    "import numpy as np                                \n",
    "import pandas as pd                               \n",
    "import matplotlib.pyplot as plt                   \n",
    "from IPython.display import Image                 \n",
    "from IPython.display import display               \n",
    "from time import gmtime, strftime                 \n",
    "from sagemaker.predictor import csv_serializer   \n",
    "\n",
    "# Define IAM role\n",
    "role = get_execution_role()\n",
    "prefix = 'sagemaker/DEMO-xgboost-dm'\n",
    "containers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest',\n",
    "              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest',\n",
    "              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest',\n",
    "              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest'} # each region has its XGBoost container\n",
    "my_region = boto3.session.Session().region_name # set the region of the instance\n",
    "print(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + containers[my_region] + \" container for your SageMaker endpoint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a SageMaker pre-built XGBoost model, you will need to reformat the header and first column of the training data and load the data from the S3 bucket. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using [Sagemaker XGBoost](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html). Also check out this [example XGBoost notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_abalone.ipynb)\n",
    "\n",
    "For CSV training, the algorithm assumes that the target variable is in the first column and that the CSV does not have a header record. For CSV inference, the algorithm assumes that CSV input does not have the label column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write training data to csv in the required XGBoost format\n",
    "pd.concat([y_train, X_train], axis=1).to_csv('./data/train.csv', index=False, header=False)\n",
    "\n",
    "# upload to bucket used for training\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train/train.csv')).upload_file('./data/train.csv')\n",
    "\n",
    "# load training data for sagemaker\n",
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket_name, prefix), content_type='csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to set up the SageMaker session, create an instance of the XGBoost model (an estimator), and define the modelâ€™s hyperparameters. \n",
    "\n",
    "Note, for a regression task you will need to set `objective='reg:linear'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "xgb = sagemaker.estimator.Estimator(containers[my_region],\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket_name, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "\n",
    "xgb.set_hyperparameters(max_depth=5,eta=0.2,gamma=4,min_child_weight=6,subsample=0.8,silent=0,objective='reg:linear',num_round=100)  # set objective function here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data loaded and XGBoost estimator set up, train the model using gradient optimization on a ml.m4.xlarge instance by executing the `fit` command.\n",
    "\n",
    "This kicks off a training job (see UI), where the training happens. Does not happen on this notebook instance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit({'train': s3_input_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model\n",
    "\n",
    "You will deploy the trained model to an endpoint, reformat then load the CSV data, then run the model to create predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count=1,instance_type='ml.m5.large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).as_matrix() #load the data into an array\n",
    "X_test_array = X_test.as_matrix()\n",
    "\n",
    "xgb_predictor.content_type = 'text/csv' # set the data type for an inference\n",
    "xgb_predictor.serializer = csv_serializer # set the serializer type\n",
    "\n",
    "predictions = xgb_predictor.predict(X_test_array).decode('utf-8') # predict!\n",
    "\n",
    "predictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an array\n",
    "print(predictions_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate your model\n",
    "\n",
    "In this step, you will evaluate the performance and accuracy of the machine learning model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(y_test, predictions_array, multioutput='variance_weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminate your resources\n",
    "\n",
    "To delete the SageMaker endpoint and possibly the objects in your S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(xgb_predictor.endpoint)\n",
    "\n",
    "# bucket_to_delete = boto3.resource('s3').Bucket(bucket_name)\n",
    "# bucket_to_delete.objects.all().delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Using a custom `sklearn` model\n",
    "\n",
    "Let's first develop a custom model, and grid search pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', Imputer()),\n",
    "    ('regressor', DecisionTreeRegressor())\n",
    "])\n",
    "\n",
    "\n",
    "params = {'regressor__max_depth': [2, 3, 4, 5]}\n",
    "\n",
    "# replace len(data) for n depending on target\n",
    "grid_search = GridSearchCV(pipeline, \n",
    "                           n_jobs=-1,\n",
    "                           param_grid=params, \n",
    "                           cv=5)\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now what?\n",
    "\n",
    "We have a model, how do we deploy it and make an endpoint?\n",
    "\n",
    "Use the pre-build sklearn image as is done in this [tutorial](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_iris/Scikit-learn%20Estimator%20Example%20With%20Batch%20Transform.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "script_path = 'pipeline.py'\n",
    "\n",
    "sklearn = SKLearn(\n",
    "    entry_point=script_path,\n",
    "    train_instance_type=\"ml.m5.large\",\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    "    hyperparameters={'regressor_max_depth': 4})  # hyperparameters that are not tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably the above is for running just a standard model. Now I have also put grid search in the `pipeline.py`, but this might be removed and should be part of the hyperparameter tuning that sagemaker offers through `HyperParameterTrainingJobs`, see [here](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-ex-tuning-job.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_input = sess.upload_data(WORK_DIRECTORY, key_prefix=\"{}/{}\".format(prefix, WORK_DIRECTORY) )\n",
    "\n",
    "# sklearn.fit({'train': train_input})\n",
    "sklearn.fit({'train': s3_input_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy the model \n",
    "\n",
    "Deploying the model to SageMaker hosting just requires a deploy call on the fitted model. This call takes an instance count and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_predictor = sklearn.deploy(initial_instance_count=1, instance_type=\"ml.m5.large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as for the xgboost inference job\n",
    "\n",
    "sklearn_predictor.content_type = 'text/csv' # set the data type for an inference\n",
    "sklearn_predictor.serializer = csv_serializer # set the serializer type\n",
    "\n",
    "sklearn_predictions = sklearn_predictor.predict(X_test_array)#.decode('utf-8') # predict!\n",
    "\n",
    "sklearn_predictions_array = np.fromstring(sklearn_predictions[1:], sep=',') # and turn the prediction into an array\n",
    "print('predictions shape:', sklearn_predictions_array.shape)\n",
    "\n",
    "print('r2_score:', r2_score(y_test, predictions_array, multioutput='variance_weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
