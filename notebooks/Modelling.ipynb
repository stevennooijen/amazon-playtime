{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sagemaker demo\n",
    "\n",
    "Sagemaker has 3 options for training and deploying models:\n",
    "1. A standard Sagemaker model, like xgboost.\n",
    "2. A custom `sklearn` model using a pre-built sagemaker scikit-learn docker image.\n",
    "3. Any other custom build model using an own custom image\n",
    "\n",
    "In this notebook we demonstrate how to do option 1 and 2, which can be done without using a `Dockerfile`. Option 3 involves a bit more engineering.\n",
    "\n",
    "For part one we start by following this [step-by-step guide](https://aws.amazon.com/getting-started/tutorials/build-train-deploy-machine-learning-model-sagemaker/) on how to model with sagemaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "bucket_name = 'playbucket-steven'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example data\n",
    "\n",
    "Here we are going to predict Black Friday sales based on this [kaggle data set](https://www.kaggle.com/mehdidag/black-friday/version/1).\n",
    "- Its around 5 MB\n",
    "- Its dimensions are 538k x 12\n",
    "\n",
    "### Getting data from S3\n",
    "\n",
    "Boto is the AWS SDK for Python. Access S3 using boto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create s3 resource\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Print out bucket names\n",
    "for bucket in s3.buckets.all():\n",
    "    print(bucket.name)\n",
    "    # print out file names in bucket, called 'keys'\n",
    "    for objects in bucket.objects.all():\n",
    "        print('- ', objects.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon S3 does not provide compute, such as zip compression/decompression. You would need to write a program that:\n",
    "- Downloads the zip file\n",
    "- Extracts the files\n",
    "- Does actions on the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "# Download if not there yet\n",
    "if not os.path.isfile('./data/black-friday.zip'):\n",
    "    s3.Bucket(bucket_name).download_file('black-friday.zip', 'data/black-friday.zip')\n",
    "\n",
    "# Extract and read with pandas\n",
    "zf = zipfile.ZipFile('data/black-friday.zip') \n",
    "df = pd.read_csv(zf.open('BlackFriday.csv'))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine and preprocess dataset\n",
    "\n",
    "Target column to predict: \t`Purchase` = Purchase amount in dollars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(537577, 12)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create X and y data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 7:] # keep only integer columns to avoid one hot encoding\n",
    "y = X.pop('Purchase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split in train test and create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Using a standard `Sagemaker` model\n",
    "\n",
    "To prepare the data, train the machine learning model, and deploy it, you will need to import some libraries and define a few environment variables in your Jupyter notebook environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success - the MySageMakerInstance is in the eu-west-1 region. You will use the 685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest container for your SageMaker endpoint.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import boto3, re, sys, math, json, os, sagemaker, urllib.request\n",
    "from sagemaker import get_execution_role\n",
    "import numpy as np                                \n",
    "import pandas as pd                               \n",
    "import matplotlib.pyplot as plt                   \n",
    "from IPython.display import Image                 \n",
    "from IPython.display import display               \n",
    "from time import gmtime, strftime                 \n",
    "from sagemaker.predictor import csv_serializer   \n",
    "\n",
    "# Define IAM role\n",
    "role = get_execution_role()\n",
    "prefix = 'sagemaker/DEMO-xgboost-dm'\n",
    "containers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest',\n",
    "              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest',\n",
    "              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest',\n",
    "              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest'} # each region has its XGBoost container\n",
    "my_region = boto3.session.Session().region_name # set the region of the instance\n",
    "print(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + containers[my_region] + \" container for your SageMaker endpoint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a SageMaker pre-built XGBoost model, you will need to reformat the header and first column of the training data and load the data from the S3 bucket. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using [Sagemaker XGBoost](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html). Also check out this [example XGBoost notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_abalone.ipynb)\n",
    "\n",
    "For CSV training, the algorithm assumes that the target variable is in the first column and that the CSV does not have a header record. For CSV inference, the algorithm assumes that CSV input does not have the label column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write training data to csv in the required XGBoost format\n",
    "pd.concat([y_train, X_train], axis=1).to_csv('./data/train.csv', index=False, header=False)\n",
    "\n",
    "# upload to bucket used for training\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train/train.csv')).upload_file('./data/train.csv')\n",
    "\n",
    "# load training data for sagemaker\n",
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket_name, prefix), content_type='csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to set up the SageMaker session, create an instance of the XGBoost model (an estimator), and define the modelâ€™s hyperparameters. \n",
    "\n",
    "Note, for a regression task you will need to set `objective='reg:linear'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "xgb = sagemaker.estimator.Estimator(containers[my_region],\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket_name, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "\n",
    "xgb.set_hyperparameters(max_depth=5,eta=0.2,gamma=4,min_child_weight=6,subsample=0.8,silent=0,objective='reg:linear',num_round=100)  # set objective function here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data loaded and XGBoost estimator set up, train the model using gradient optimization on a ml.m4.xlarge instance by executing the `fit` command.\n",
    "\n",
    "This kicks off a training job (see UI), where the training happens. Does not happen on this notebook instance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: xgboost-2019-04-05-09-29-20-327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-05 09:29:20 Starting - Starting the training job...\n",
      "2019-04-05 09:29:21 Starting - Launching requested ML instances......\n",
      "2019-04-05 09:30:24 Starting - Preparing the instances for training......\n",
      "2019-04-05 09:31:45 Downloading - Downloading input data\n",
      "2019-04-05 09:31:45 Training - Downloading the training image.\n",
      "\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[31m[2019-04-05:09:31:53:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[31m[2019-04-05:09:31:53:INFO] Path /opt/ml/input/data/validation does not exist!\u001b[0m\n",
      "\u001b[31m[2019-04-05:09:31:53:INFO] File size need to be processed in the node: 5.15mb. Available memory size in the node: 8388.12mb\u001b[0m\n",
      "\u001b[31m[2019-04-05:09:31:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m[09:31:53] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[31m[09:31:53] 360176x4 matrix with 1190311 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[31m[09:31:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[0]#011train-rmse:8723.29\u001b[0m\n",
      "\u001b[31m[09:31:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[1]#011train-rmse:7234.66\u001b[0m\n",
      "\u001b[31m[09:31:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[2]#011train-rmse:6092.91\u001b[0m\n",
      "\u001b[31m[09:31:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[3]#011train-rmse:5230.34\u001b[0m\n",
      "\u001b[31m[09:31:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[4]#011train-rmse:4594.07\u001b[0m\n",
      "\u001b[31m[09:31:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[5]#011train-rmse:4134.08\u001b[0m\n",
      "\u001b[31m[09:31:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[6]#011train-rmse:3808.49\u001b[0m\n",
      "\u001b[31m[09:31:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[7]#011train-rmse:3536.87\u001b[0m\n",
      "\u001b[31m[09:31:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[8]#011train-rmse:3350.11\u001b[0m\n",
      "\u001b[31m[09:31:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[9]#011train-rmse:3224.46\u001b[0m\n",
      "\u001b[31m[09:31:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[10]#011train-rmse:3139.85\u001b[0m\n",
      "\u001b[31m[09:31:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[11]#011train-rmse:3083.57\u001b[0m\n",
      "\u001b[31m[09:31:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[12]#011train-rmse:3045.93\u001b[0m\n",
      "\u001b[31m[09:31:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[13]#011train-rmse:3021.58\u001b[0m\n",
      "\u001b[31m[09:31:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[14]#011train-rmse:3004.91\u001b[0m\n",
      "\u001b[31m[09:31:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[15]#011train-rmse:2993.32\u001b[0m\n",
      "\u001b[31m[09:31:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[16]#011train-rmse:2986.15\u001b[0m\n",
      "\u001b[31m[09:31:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[17]#011train-rmse:2981.42\u001b[0m\n",
      "\u001b[31m[09:31:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[18]#011train-rmse:2977.65\u001b[0m\n",
      "\u001b[31m[09:31:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[19]#011train-rmse:2974.77\u001b[0m\n",
      "\u001b[31m[09:31:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[20]#011train-rmse:2972.69\u001b[0m\n",
      "\u001b[31m[09:31:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[21]#011train-rmse:2971.02\u001b[0m\n",
      "\u001b[31m[09:31:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[22]#011train-rmse:2970.11\u001b[0m\n",
      "\u001b[31m[09:31:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[23]#011train-rmse:2969.1\u001b[0m\n",
      "\u001b[31m[09:31:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[24]#011train-rmse:2967.69\u001b[0m\n",
      "\u001b[31m[09:31:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[25]#011train-rmse:2966.61\u001b[0m\n",
      "\u001b[31m[09:31:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[26]#011train-rmse:2965.17\u001b[0m\n",
      "\u001b[31m[09:31:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[27]#011train-rmse:2964.08\u001b[0m\n",
      "\u001b[31m[09:31:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[28]#011train-rmse:2963.72\u001b[0m\n",
      "\u001b[31m[09:31:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[29]#011train-rmse:2963.09\u001b[0m\n",
      "\u001b[31m[09:31:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[30]#011train-rmse:2962.19\u001b[0m\n",
      "\u001b[31m[09:31:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[31]#011train-rmse:2961.88\u001b[0m\n",
      "\u001b[31m[09:31:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[32]#011train-rmse:2961.73\u001b[0m\n",
      "\u001b[31m[09:31:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[33]#011train-rmse:2960.99\u001b[0m\n",
      "\u001b[31m[09:31:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[34]#011train-rmse:2959.69\u001b[0m\n",
      "\u001b[31m[09:31:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[35]#011train-rmse:2959.47\u001b[0m\n",
      "\u001b[31m[09:31:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[36]#011train-rmse:2958.22\u001b[0m\n",
      "\u001b[31m[09:31:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[37]#011train-rmse:2957.34\u001b[0m\n",
      "\u001b[31m[09:31:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[38]#011train-rmse:2956.76\u001b[0m\n",
      "\u001b[31m[09:31:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[39]#011train-rmse:2956.67\u001b[0m\n",
      "\u001b[31m[09:31:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[40]#011train-rmse:2955.58\u001b[0m\n",
      "\u001b[31m[09:31:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[41]#011train-rmse:2954.87\u001b[0m\n",
      "\u001b[31m[09:31:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[42]#011train-rmse:2954.2\u001b[0m\n",
      "\u001b[31m[09:31:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[43]#011train-rmse:2953.97\u001b[0m\n",
      "\u001b[31m[09:31:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[44]#011train-rmse:2953.26\u001b[0m\n",
      "\u001b[31m[09:31:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[45]#011train-rmse:2953.12\u001b[0m\n",
      "\u001b[31m[09:31:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[46]#011train-rmse:2952.61\u001b[0m\n",
      "\u001b[31m[09:31:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[47]#011train-rmse:2952.3\u001b[0m\n",
      "\u001b[31m[09:31:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[48]#011train-rmse:2952.2\u001b[0m\n",
      "\u001b[31m[09:31:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[49]#011train-rmse:2951.94\u001b[0m\n",
      "\u001b[31m[09:31:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[50]#011train-rmse:2951.82\u001b[0m\n",
      "\u001b[31m[09:31:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[51]#011train-rmse:2951.75\u001b[0m\n",
      "\u001b[31m[09:31:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[52]#011train-rmse:2951.58\u001b[0m\n",
      "\u001b[31m[09:31:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[53]#011train-rmse:2951.37\u001b[0m\n",
      "\u001b[31m[09:31:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[54]#011train-rmse:2951.17\u001b[0m\n",
      "\u001b[31m[09:31:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[55]#011train-rmse:2951.03\u001b[0m\n",
      "\u001b[31m[09:31:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[56]#011train-rmse:2950.73\u001b[0m\n",
      "\u001b[31m[09:31:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[57]#011train-rmse:2950.55\u001b[0m\n",
      "\u001b[31m[09:31:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[58]#011train-rmse:2950.26\u001b[0m\n",
      "\u001b[31m[09:31:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[59]#011train-rmse:2950.11\u001b[0m\n",
      "\u001b[31m[09:31:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[60]#011train-rmse:2949.78\u001b[0m\n",
      "\u001b[31m[09:31:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[61]#011train-rmse:2949.71\u001b[0m\n",
      "\u001b[31m[09:31:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[62]#011train-rmse:2949.53\u001b[0m\n",
      "\u001b[31m[09:31:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[63]#011train-rmse:2949.29\u001b[0m\n",
      "\u001b[31m[09:31:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[64]#011train-rmse:2949.08\u001b[0m\n",
      "\u001b[31m[09:31:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[65]#011train-rmse:2948.94\u001b[0m\n",
      "\u001b[31m[09:31:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[66]#011train-rmse:2948.86\u001b[0m\n",
      "\u001b[31m[09:31:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[67]#011train-rmse:2948.66\u001b[0m\n",
      "\u001b[31m[09:31:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[68]#011train-rmse:2948.49\u001b[0m\n",
      "\u001b[31m[09:31:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[69]#011train-rmse:2948.46\u001b[0m\n",
      "\u001b[31m[09:31:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[70]#011train-rmse:2948.25\u001b[0m\n",
      "\u001b[31m[09:31:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[71]#011train-rmse:2948.11\u001b[0m\n",
      "\u001b[31m[09:31:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[72]#011train-rmse:2948.02\u001b[0m\n",
      "\u001b[31m[09:31:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[73]#011train-rmse:2947.55\u001b[0m\n",
      "\u001b[31m[09:31:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[74]#011train-rmse:2947.51\u001b[0m\n",
      "\u001b[31m[09:31:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[75]#011train-rmse:2947.47\u001b[0m\n",
      "\u001b[31m[09:31:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[76]#011train-rmse:2947.38\u001b[0m\n",
      "\u001b[31m[09:32:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[77]#011train-rmse:2947.29\u001b[0m\n",
      "\u001b[31m[09:32:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[78]#011train-rmse:2947.2\u001b[0m\n",
      "\u001b[31m[09:32:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[79]#011train-rmse:2947.1\u001b[0m\n",
      "\u001b[31m[09:32:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[80]#011train-rmse:2947.05\u001b[0m\n",
      "\u001b[31m[09:32:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[81]#011train-rmse:2946.99\u001b[0m\n",
      "\u001b[31m[09:32:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[82]#011train-rmse:2946.89\u001b[0m\n",
      "\u001b[31m[09:32:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[83]#011train-rmse:2946.84\u001b[0m\n",
      "\u001b[31m[09:32:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[84]#011train-rmse:2946.74\u001b[0m\n",
      "\u001b[31m[09:32:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[85]#011train-rmse:2946.73\u001b[0m\n",
      "\u001b[31m[09:32:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[86]#011train-rmse:2946.71\u001b[0m\n",
      "\u001b[31m[09:32:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[87]#011train-rmse:2946.6\u001b[0m\n",
      "\u001b[31m[09:32:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[88]#011train-rmse:2946.51\u001b[0m\n",
      "\u001b[31m[09:32:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[89]#011train-rmse:2946.4\u001b[0m\n",
      "\u001b[31m[09:32:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[90]#011train-rmse:2946.29\u001b[0m\n",
      "\u001b[31m[09:32:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[91]#011train-rmse:2946.28\u001b[0m\n",
      "\u001b[31m[09:32:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[92]#011train-rmse:2946.25\u001b[0m\n",
      "\u001b[31m[09:32:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[93]#011train-rmse:2946.11\u001b[0m\n",
      "\u001b[31m[09:32:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[94]#011train-rmse:2946.03\u001b[0m\n",
      "\u001b[31m[09:32:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[95]#011train-rmse:2945.83\u001b[0m\n",
      "\u001b[31m[09:32:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[96]#011train-rmse:2945.77\u001b[0m\n",
      "\u001b[31m[09:32:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[97]#011train-rmse:2945.73\u001b[0m\n",
      "\u001b[31m[09:32:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[98]#011train-rmse:2945.7\u001b[0m\n",
      "\u001b[31m[09:32:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[99]#011train-rmse:2945.67\u001b[0m\n",
      "\n",
      "2019-04-05 09:32:10 Uploading - Uploading generated training model\n",
      "2019-04-05 09:32:10 Completed - Training job completed\n",
      "Billable seconds: 45\n"
     ]
    }
   ],
   "source": [
    "xgb.fit({'train': s3_input_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model\n",
    "\n",
    "You will deploy the trained model to an endpoint, reformat then load the CSV data, then run the model to create predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: xgboost-2019-04-05-09-38-06-958\n",
      "INFO:sagemaker:Creating endpoint with name xgboost-2019-04-05-09-29-20-327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count=1,instance_type='ml.m5.large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(177401,)\n"
     ]
    }
   ],
   "source": [
    "# test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).as_matrix() #load the data into an array\n",
    "X_test_array = X_test.as_matrix()\n",
    "\n",
    "xgb_predictor.content_type = 'text/csv' # set the data type for an inference\n",
    "xgb_predictor.serializer = csv_serializer # set the serializer type\n",
    "\n",
    "predictions = xgb_predictor.predict(X_test_array).decode('utf-8') # predict!\n",
    "\n",
    "predictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an array\n",
    "print(predictions_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate your model\n",
    "\n",
    "In this step, you will evaluate the performance and accuracy of the machine learning model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.646891500834572"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(y_test, predictions_array, multioutput='variance_weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminate your resources\n",
    "\n",
    "To delete the SageMaker endpoint and possibly the objects in your S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: xgboost-2019-04-05-09-29-20-327\n"
     ]
    }
   ],
   "source": [
    "sagemaker.Session().delete_endpoint(xgb_predictor.endpoint)\n",
    "\n",
    "# bucket_to_delete = boto3.resource('s3').Bucket(bucket_name)\n",
    "# bucket_to_delete.objects.all().delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Using a custom `sklearn` model\n",
    "\n",
    "Let's first develop a custom model, and grid search pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('regressor', DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best'))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'regressor__max_depth': [2, 3, 4, 5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', Imputer()),\n",
    "    ('regressor', DecisionTreeRegressor())\n",
    "])\n",
    "\n",
    "\n",
    "params = {'regressor__max_depth': [2, 3, 4, 5]}\n",
    "\n",
    "# replace len(data) for n depending on target\n",
    "grid_search = GridSearchCV(pipeline, \n",
    "                           n_jobs=-1,\n",
    "                           param_grid=params, \n",
    "                           cv=5)\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13330.77036981, 13330.77036981,  8976.93940026, ...,\n",
       "       13780.45761711,  4710.39645491,  6412.51659019])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.493346139146699"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now what?\n",
    "\n",
    "We have a model, how do we deploy it and make an endpoint?\n",
    "\n",
    "Use the pre-build sklearn image as is done in this [tutorial](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_iris/Scikit-learn%20Estimator%20Example%20With%20Batch%20Transform.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "script_path = 'pipeline.py'\n",
    "\n",
    "sklearn = SKLearn(\n",
    "    entry_point=script_path,\n",
    "    train_instance_type=\"ml.m5.large\",\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    "    hyperparameters={'regressor_max_depth': 4})  # hyperparameters that are not tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably the above is for running just a standard model. Now I have also put grid search in the `pipeline.py`, but this might be removed and should be part of the hyperparameter tuning that sagemaker offers through `HyperParameterTrainingJobs`, see [here](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-ex-tuning-job.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-scikit-learn-2019-04-05-14-25-08-068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-05 14:25:08 Starting - Starting the training job...\n",
      "2019-04-05 14:25:09 Starting - Launching requested ML instances......\n",
      "2019-04-05 14:26:11 Starting - Preparing the instances for training......\n",
      "2019-04-05 14:27:35 Downloading - Downloading input data\n",
      "2019-04-05 14:27:35 Training - Training image download completed. Training in progress..\n",
      "\u001b[31m2019-04-05 14:27:35,369 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[31m2019-04-05 14:27:35,372 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-04-05 14:27:35,394 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[31m2019-04-05 14:27:35,756 sagemaker-containers INFO     Module pipeline does not provide a setup.py. \u001b[0m\n",
      "\u001b[31mGenerating setup.py\u001b[0m\n",
      "\u001b[31m2019-04-05 14:27:35,757 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[31m2019-04-05 14:27:35,757 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m2019-04-05 14:27:35,758 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[31m/usr/bin/python3 -m pip install -U . \u001b[0m\n",
      "\u001b[31mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[31mBuilding wheels for collected packages: pipeline\n",
      "  Running setup.py bdist_wheel for pipeline: started\u001b[0m\n",
      "\u001b[31m  Running setup.py bdist_wheel for pipeline: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-56ru3nkx/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[31mSuccessfully built pipeline\u001b[0m\n",
      "\u001b[31mInstalling collected packages: pipeline\u001b[0m\n",
      "\u001b[31mSuccessfully installed pipeline-1.0.0\u001b[0m\n",
      "\u001b[31mYou are using pip version 18.1, however version 19.0.3 is available.\u001b[0m\n",
      "\u001b[31mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31m2019-04-05 14:27:37,630 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-04-05 14:27:37,649 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"RecordWrapperType\": \"None\",\n",
      "            \"ContentType\": \"csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\"\n",
      "        }\n",
      "    },\n",
      "    \"user_entry_point\": \"pipeline.py\",\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"num_cpus\": 2,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-226214933900/sagemaker-scikit-learn-2019-04-05-14-25-08-068/source/sourcedir.tar.gz\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"job_name\": \"sagemaker-scikit-learn-2019-04-05-14-25-08-068\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"module_name\": \"pipeline\",\n",
      "    \"log_level\": 20,\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"resource_config\": {\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"network_interface_name\": \"ethwe\"\n",
      "    },\n",
      "    \"hyperparameters\": {\n",
      "        \"regressor_max_depth\": 4\n",
      "    },\n",
      "    \"network_interface_name\": \"ethwe\",\n",
      "    \"num_gpus\": 0,\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"input_dir\": \"/opt/ml/input\"\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"regressor_max_depth\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-scikit-learn-2019-04-05-14-25-08-068\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-226214933900/sagemaker-scikit-learn-2019-04-05-14-25-08-068/source/sourcedir.tar.gz\",\"module_name\":\"pipeline\",\"network_interface_name\":\"ethwe\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"ethwe\"},\"user_entry_point\":\"pipeline.py\"}\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://sagemaker-eu-west-1-226214933900/sagemaker-scikit-learn-2019-04-05-14-25-08-068/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[31mSM_HP_REGRESSOR_MAX_DEPTH=4\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=pipeline\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=ethwe\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[31mSM_HPS={\"regressor_max_depth\":4}\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=2\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"ethwe\"}\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[\"--regressor_max_depth\",\"4\"]\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_USER_ENTRY_POINT=pipeline.py\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/bin/python3 -m pipeline --regressor_max_depth 4\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.5/dist-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\u001b[0m\n",
      "\u001b[31m2019-04-05 14:27:40,675 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2019-04-05 14:27:47 Uploading - Uploading generated training model\n",
      "2019-04-05 14:27:47 Completed - Training job completed\n",
      "Billable seconds: 22\n"
     ]
    }
   ],
   "source": [
    "# train_input = sess.upload_data(WORK_DIRECTORY, key_prefix=\"{}/{}\".format(prefix, WORK_DIRECTORY) )\n",
    "\n",
    "# sklearn.fit({'train': train_input})\n",
    "sklearn.fit({'train': s3_input_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy the model \n",
    "\n",
    "Deploying the model to SageMaker hosting just requires a deploy call on the fitted model. This call takes an instance count and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-scikit-learn-2019-04-05-14-25-08-068\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-scikit-learn-2019-04-05-14-25-08-068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "sklearn_predictor = sklearn.deploy(initial_instance_count=1, instance_type=\"ml.m5.large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions shape: (0,)\n",
      "r2_score: 0.646891500834572\n"
     ]
    }
   ],
   "source": [
    "# same as for the xgboost inference job\n",
    "\n",
    "sklearn_predictor.content_type = 'text/csv' # set the data type for an inference\n",
    "sklearn_predictor.serializer = csv_serializer # set the serializer type\n",
    "\n",
    "sklearn_predictions = sklearn_predictor.predict(X_test_array)#.decode('utf-8') # predict!\n",
    "\n",
    "sklearn_predictions_array = np.fromstring(sklearn_predictions[1:], sep=',') # and turn the prediction into an array\n",
    "print('predictions shape:', sklearn_predictions_array.shape)\n",
    "\n",
    "print('r2_score:', r2_score(y_test, predictions_array, multioutput='variance_weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
